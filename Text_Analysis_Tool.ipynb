{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP98InT1Pas1HJSTtjLffNe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManuWalls/Artificial_Intelligence-/blob/main/Text_Analysis_Tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manuel Paredes SÃ¡nchez #1953821\n",
        "\n",
        "**Text Analysis Tool:** Create a Python program that reads a text file and performs various analyses on its content. The program should: Count the number of words, sentences, and paragraphs. Calculate the average word length. Identify the most common words and their frequencies."
      ],
      "metadata": {
        "id": "b8rxuqtvxHbe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVF0fb2i-D80",
        "outputId": "452cbf1e-9af5-40d0-9634-3941a93736f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count: 857\n",
            "Sentence count: 32\n",
            "Paragraph count: 9\n",
            "Average word length: 5.08051341890315\n",
            "\n",
            "Most common words:\n",
            ",: 45\n",
            "the: 40\n",
            "of: 33\n",
            ".: 32\n",
            "and: 27\n",
            "to: 20\n",
            "that: 16\n",
            "in: 14\n",
            "AI: 10\n",
            "The: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# The Natural Language Toolkit (NLTK) is a Phyton library that is primarily\n",
        "# used for working with human language data, also known as natural language\n",
        "# processing (NLP). NLTK provides a wide range of tools and resources for\n",
        "# various tasks related to analyzing and processing text data.\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize # tokenization: process of splitting text into sentences and words.\n",
        "# sent_tokenize: is used to tokenize text into sentences.\n",
        "# word_tokenize: is used to tokenize text into words.\n",
        "from nltk.probability import FreqDist\n",
        "# FreqDist is used for frequency distribution analysis. Counting frequency of words in a text.\n",
        "\n",
        "# Download NLTK data if not already downloaded\n",
        "nltk.download('punkt') # Punkt tokenizer models, these models are used for tokenization tasks.\n",
        "\n",
        "# Function to count words, sentences, and paragraphs\n",
        "def count_stats(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    return len(words), len(sentences), len(paragraphs)\n",
        "\n",
        "# Function to calculate the average word length\n",
        "def average_word_length(text):\n",
        "    words = word_tokenize(text)\n",
        "    total_word_length = sum(len(word) for word in words)\n",
        "    return total_word_length / len(words)\n",
        "\n",
        "# Function to identify the most common words and their frequencies\n",
        "def most_common_words(text, n=10):\n",
        "    words = word_tokenize(text)\n",
        "    fdist = FreqDist(words)\n",
        "    return fdist.most_common(n)\n",
        "\n",
        "# Main program\n",
        "if __name__ == \"__main__\":\n",
        "    filename = \"Prueba1.txt\"  # Replace with the path to your text file\n",
        "\n",
        "    try:\n",
        "        with open(filename, \"r\") as file:\n",
        "            text = file.read()\n",
        "\n",
        "        word_count, sentence_count, paragraph_count = count_stats(text)\n",
        "        avg_word_length = average_word_length(text)\n",
        "        common_words = most_common_words(text)\n",
        "\n",
        "        print(\"Word count:\", word_count)\n",
        "        print(\"Sentence count:\", sentence_count)\n",
        "        print(\"Paragraph count:\", paragraph_count)\n",
        "        print(\"Average word length:\", avg_word_length)\n",
        "        print(\"\\nMost common words:\")\n",
        "        for word, freq in common_words:\n",
        "            print(f\"{word}: {freq}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{filename}' not found.\")"
      ]
    }
  ]
}